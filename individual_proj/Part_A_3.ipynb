{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the optimal number of hidden neurons for the first depth and widths of the neural network designed in Question 1 and 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the mean cross-validation accuracies on the final epoch for at least 8 different combinations of different depth (limit to 1-3 layers) and widths (limit to 64, 128 or 256 neurons) using a scatter plot. Continue using 5-fold cross validation on the training dataset. Select the optimal number of neurons for the hidden layer. State the rationale for your selection. Plot the train and test accuracies against training epochs with the optimal number of neurons using a line plot. [optional + 2 marks] Implement an alternative approach that searches through these combinations that could significantly reduce the computational time but achieve similar search results, without enumeration all the possibilities.\n",
    "\n",
    "\n",
    "\n",
    "This might take a while to run, approximately 30 - 60 min, so plan your time carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Firstly, we import relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from scipy.io import wavfile as wav\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "from common_utils import set_seed\n",
    "\n",
    "# setting seed\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.To reduce repeated code, place your\n",
    "\n",
    "- network (MLP defined in QA1)\n",
    "- torch datasets (CustomDataset defined in QA1)\n",
    "- loss function (loss_fn defined in QA1)\n",
    "\n",
    "in a separate file called **common_utils.py**\n",
    "\n",
    "Import them into this file. You will not be repenalised for any error in QA1 here as the code in QA1 will not be remarked.\n",
    "\n",
    "The following code cell will not be marked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from common_utils import MLP, split_dataset, preprocess_dataset, CustomDataset, loss_fn, preprocess, EarlyStopper\n",
    "import pandas as pd\n",
    "\n",
    "# redefine MLP to create depth \n",
    "\n",
    "class MLP(MLP):\n",
    "    def __init__(self, no_features, num_neurons, depth, no_labels):\n",
    "        super().__init__(no_features, num_neurons, no_labels)\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(no_features, num_neurons))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(0.2))\n",
    "\n",
    "        for _ in range(depth - 1):\n",
    "            layers.append(nn.Linear(num_neurons, num_neurons))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.2))\n",
    "\n",
    "        layers.append(nn.Linear(num_neurons, no_labels))\n",
    "        layers.append(nn.Sigmoid())\n",
    "\n",
    "        self.mlp_stack = nn.Sequential(*layers)\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    train_loss, correct = 0, 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # FP\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        train_loss += loss.item()\n",
    "        correct += ((pred > 0.5).type(torch.float) == y).type(torch.float).sum().item()\n",
    "\n",
    "        # BP\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    \n",
    "    train_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Train Error: \\n Accuracy: {(correct*100):>0.1f}%, Avg loss: {train_loss:>8f} \\n\")\n",
    "    \n",
    "    return train_loss, correct\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += ((pred > 0.5).type(torch.float) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "    return test_loss, correct\n",
    "\n",
    "df = pd.read_csv('simplified.csv')\n",
    "df['label'] = df['filename'].str.split('_').str[-2]\n",
    "\n",
    "X_train, y_train, X_test, y_test = preprocess(df)\n",
    "\n",
    "train_data = CustomDataset(X_train, y_train)\n",
    "test_data = CustomDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Perform hyperparameter tuning for the different neurons with 5-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train_scaled, y_train2, X_val_scaled, y_val2, batch_size):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    train_data = CustomDataset(X_train_scaled, y_train2)\n",
    "    val_data = CustomDataset(X_val_scaled, y_val2)\n",
    "\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    early_stopper = EarlyStopper(patience=3, min_delta=0)\n",
    "\n",
    "    train_accuracies = []\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "    test_losses = []\n",
    "\n",
    "    times = []\n",
    "\n",
    "    for t in range(100):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        \n",
    "        # Train\n",
    "        start = time.time()\n",
    "        train_loss, train_acc = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "\n",
    "        # Log train\n",
    "        train_accuracies.append(train_acc), train_losses.append(train_loss)\n",
    "\n",
    "        # Validation\n",
    "        test_loss, test_acc = test_loop(val_dataloader, model, loss_fn)\n",
    "        end = time.time()\n",
    "        \n",
    "        # Log test\n",
    "        times.append(end-start)\n",
    "        test_accuracies.append(test_acc), test_losses.append(test_loss)\n",
    "\n",
    "        if early_stopper.early_stop(test_loss):\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "    return train_accuracies, train_losses, test_accuracies, test_losses, times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_hyperparameter(X_train, y_train, parameters, mode, batch_size):\n",
    "    # YOUR CODE HERE\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    \n",
    "    cross_validation_accuracies = []\n",
    "    cross_validation_times = []\n",
    "\n",
    "    for parameter in parameters:\n",
    "        print(f\"Parameter {parameter}\")\n",
    "        accuracies = []\n",
    "        times = []\n",
    "        depth = parameter[0]\n",
    "        num_neuron = parameter[1]\n",
    "        for train_index, val_index in kf.split(X_train):\n",
    "            X_train2, X_val2 = X_train[train_index], X_train[val_index]\n",
    "            y_train2, y_val2 = y_train[train_index], y_train[val_index]\n",
    "\n",
    "            model = MLP(77,num_neuron,depth,1)\n",
    "\n",
    "            _, _, test_accuracies, _, time = train(model, X_train2, y_train2, X_val2, y_val2, batch_size)\n",
    "            \n",
    "            # Save the accuracy for each fold at the last epoch\n",
    "            accuracies.append(test_accuracies[-1])\n",
    "            \n",
    "            # Save the time taken to train for each fold at the last epoch\n",
    "            times.append(time[-1])\n",
    "            \n",
    "        # Mean Accuracy for the Number of Neurons (Mean of Accuracy at Last Epoch for each Fold)\n",
    "        cross_validation_accuracies.append(np.mean(accuracies))\n",
    "\n",
    "        # Mean Time taken for the Number of Neurons (Mean of Time at Last Epoch for each Fold)\n",
    "        cross_validation_times.append(np.mean(times))\n",
    "\n",
    "    return cross_validation_accuracies, cross_validation_times\n",
    "\n",
    "'''\n",
    "optimal_bs = 0. Fill your optimal batch size in the following code.\n",
    "'''\n",
    "# YOUR CODE HERE\n",
    "optimal_bs = 512\n",
    "num_neurons = [64,128,256]\n",
    "depths = [1,2,3]\n",
    "parameters = [(d, n) for d in depths for n in num_neurons]\n",
    "cross_validation_accuracies, cross_validation_times = find_optimal_hyperparameter(X_train, y_train, parameters, 'num_neurons', optimal_bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The number of considered combination is {len(parameters)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Plot the mean cross-validation accuracies on the final epoch for at least 8 different combinations of different depth (limit to 1-3 layers) and widths (limit to 64, 128 or 256 neurons) using a scatter plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "plt.plot(num_neurons, cross_validation_accuracies, marker='x', linestyle='None')\n",
    "plt.xlabel('Number of neurons')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs Number of neurons')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "plt.plot(depths, cross_validation_accuracies, marker='x', linestyle='None')\n",
    "plt.xlabel('Number of depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs Depth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Select the optimal combination for the depth and width. State the rationale for your selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_combination = [3,256]\n",
    "reason = \"It seems that for this combination, the model performs the best. Depth 1 is insufficient to capture the complexity of the data, and the model is underfitting. Depth 2 and 3 perform similarly, but depth 3 has a higher accuracy. The number of neurons 256 is the best performing number of neurons, as it has the highest accuracy.\"\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.Plot the train and test accuracies against training epochs with the optimal number of neurons using a line plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "df = pd.read_csv('simplified.csv')\n",
    "df['label'] = df['filename'].str.split('_').str[-2]\n",
    "\n",
    "X_train, y_train, X_test, y_test = preprocess(df)\n",
    "\n",
    "input_features = X_train.shape[1]\n",
    "no_labels = 1\n",
    "final_model = MLP(input_features,optimal_combination[1],optimal_combination[0],no_labels)\n",
    "optimal_bs = 256\n",
    "\n",
    "train_accuracies, train_losses, test_accuracies, test_losses, times = train(final_model, X_train, y_train, X_test, y_test, optimal_bs)\n",
    "\n",
    "# save the model\n",
    "torch.save(final_model.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.As you've astutely observed, we're facing a significant challenge in enumerating all possible combinations of widths and depths and searching over them. Given the circumstances, could you explore and implement a more efficient method for searching through these combinations that could significantly reduce the computational time but achieve similar search results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "answer = \"\"\"\n",
    "One naive way is to do grid serach, but that would be greatly inefficient. The\n",
    "better alternative is to use random search, where we randomly sample the hyperparameters.\n",
    "This often performs better than grid search.\n",
    "\n",
    "Searching on the internet, we find that there is indeed a technique called Bayesian Optimization\n",
    "which is used to optimize hyperparameters. It is a sequential model-based optimization technique\n",
    "that uses the past evaluations to determine the next hyperparameters to evaluate. This is more efficient\n",
    "than random search and grid search.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
