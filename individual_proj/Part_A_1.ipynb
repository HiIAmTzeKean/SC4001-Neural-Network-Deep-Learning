{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question A1 (15 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Design a feedforward deep neural network (DNN) which consists of **three** hidden layers of 128 neurons each with ReLU activation function, and an output layer with sigmoid activation function. Apply dropout of probability **0.2** to each of the hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from scipy.io import wavfile as wav\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "from common_utils import set_seed\n",
    "\n",
    "# setting seed\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Define the model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, no_features, no_hidden, no_labels):\n",
    "        super().__init__()\n",
    "        self.mlp_stack = nn.Sequential(\n",
    "            nn.Linear(no_features, no_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(no_hidden, no_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(no_hidden, no_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(no_hidden, no_labels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divide the dataset into a 80:20 ratio for training and testing. Use **appropriate** scaling of input features. We solely assume that there are only two datasets here: training & test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Split the dataset and do preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils import split_dataset, preprocess_dataset\n",
    "\n",
    "\n",
    "def preprocess(df, test_size=0.2, random_state=42):\n",
    "    df_train, y_train, df_test, y_test = split_dataset(df, [\"filename\"], test_size, random_state)\n",
    "    X_train_scaled, X_test_scaled = preprocess_dataset(df_train, df_test)\n",
    "    return X_train_scaled, y_train, X_test_scaled, y_test\n",
    "\n",
    "df = pd.read_csv('simplified.csv')\n",
    "df['label'] = df['filename'].str.split('_').str[-2]\n",
    "\n",
    "df['label'].value_counts()\n",
    "\n",
    "X_train_scaled, y_train, X_test_scaled, y_test = preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the training dataset to train the model for 100 epochs. Use a mini-batch gradient descent with **‘Adam’** optimizer with learning rate of **0.001**, and **batch size = 128**. Implement early stopping with patience of **3**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Define a Pytorch Dataset and Dataloaders.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "def intialise_loaders(X_train_scaled, y_train, X_test_scaled, y_test, batch_size=128):\n",
    "    train_dataset = CustomDataset(X_train_scaled, y_train)\n",
    "    test_dataset = CustomDataset(X_test_scaled, y_test)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "train_dataloader, test_dataloader = intialise_loaders(X_train_scaled, y_train, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Next, define the model, optimizer and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "model = MLP(X_train_scaled.shape[1], 128, len(df['label'].unique()))\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.Train model for 100 epochs. Record down train and test accuracies. Implement early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Acc: 99.96%, Test Loss: 0.3139\n",
      "Epoch 2: Validation Acc: 99.92%, Test Loss: 0.3138\n",
      "Epoch 3: Validation Acc: 100.00%, Test Loss: 0.3133\n",
      "Epoch 4: Validation Acc: 100.00%, Test Loss: 0.3133\n",
      "Epoch 5: Validation Acc: 100.00%, Test Loss: 0.3133\n",
      "Epoch 6: Validation Acc: 99.96%, Test Loss: 0.3135\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Create a tensorboard writer for visualization\n",
    "writer = SummaryWriter('runs/your_run')\n",
    "\n",
    "# Training loop\n",
    "best_val_acc = 0\n",
    "patience = 3\n",
    "epochs_since_improvement = 0\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # Log training loss to tensorboard\n",
    "        writer.add_scalar('Loss/train', loss.item(), epoch * len(train_dataloader) + batch_idx)\n",
    "\n",
    "    # Evaluation on validation set\n",
    "    model.eval()\n",
    "    test_loss = 0  # Initialize test loss\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_dataloader:\n",
    "            output = model(data)\n",
    "            test_loss += loss_fn(output, target).item()  # Accumulate test loss\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "    # Calculate average test loss\n",
    "    test_loss /= len(test_dataloader)\n",
    "    val_acc = 100 * correct / total\n",
    "    print(f'Epoch {epoch+1}: Validation Acc: {val_acc:.2f}%, Test Loss: {test_loss:.4f}')\n",
    "\n",
    "    # Log test loss to tensorboard\n",
    "    writer.add_scalar('Loss/test', test_loss, epoch)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        epochs_since_improvement = 0\n",
    "        torch.save(model.state_dict(), f'models/best_model_{epoch}_a1.pth')\n",
    "    else:\n",
    "        epochs_since_improvement += 1\n",
    "        if epochs_since_improvement >= patience:\n",
    "            print('Early stopping')\n",
    "            break\n",
    "\n",
    "# Close tensorboard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot train and test accuracies and losses on training and test data against training epochs and comment on the line plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "I1009 20:23:57.578763 6425702400 plugin.py:429] Monitor runs begin\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.18.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "!tensorboard --logdir runs/your_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](loss_train_test.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.Comment on line plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "answer = \"\"\"\n",
    "As the loss of the model on the train set decreases the loss on the\n",
    "test set also decreases. However, cross a certain point the loss on the test set\n",
    "starts to increase which can be seen from epoch 4. By implementing early stopping\n",
    "with a patient of 3, the model stops training at epoch 5.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
